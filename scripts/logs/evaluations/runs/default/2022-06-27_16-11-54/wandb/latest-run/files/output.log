[[36m2022-06-27 16:12:18,594[39m][[34msrc.testing_pipeline[39m][[32mINFO[39m] - Instantiating trainer <pytorch_lightning.Trainer>
[[36m2022-06-27 16:12:18,717[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Using 16bit native Automatic Mixed Precision (AMP)
[[36m2022-06-27 16:12:18,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - GPU available: True, used: True
[[36m2022-06-27 16:12:18,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - TPU available: False, using: 0 TPU cores
[[36m2022-06-27 16:12:18,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - IPU available: False, using: 0 IPUs
[[36m2022-06-27 16:12:18,719[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - HPU available: False, using: 0 HPUs
[[36m2022-06-27 16:12:18,720[39m][[34msrc.testing_pipeline[39m][[32mINFO[39m] - Starting testing!
[[36m2022-06-27 16:12:30,400[39m][[34mpytorch_lightning.utilities.seed[39m][[32mINFO[39m] - Global seed set to 42
[[36m2022-06-27 16:12:30,402[39m][[34mpytorch_lightning.utilities.distributed[39m][[32mINFO[39m] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
[[36m2022-06-27 16:13:05,301[39m][[34mtorch.distributed.distributed_c10d[39m][[32mINFO[39m] - Added key: store_based_barrier_key:1 to store for rank: 0
[[36m2022-06-27 16:13:05,302[39m][[34mtorch.distributed.distributed_c10d[39m][[32mINFO[39m] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
[[36m2022-06-27 16:13:05,302[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------
[[36m2022-06-27 16:13:17,331[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Restoring states from the checkpoint path at /home/compu/jh/project/colon_compare/scripts/logs/experiments/runs/default/2022-06-27_09-45-08/checkpoints/epoch_025.ckpt
Error executing job with overrides: ['logger.wandb.tags=[tripletGL,test]', 'ckpt_path=/home/compu/jh/project/colon_compare/scripts/logs/experiments/runs/default/2022-06-27_09-45-08/checkpoints/epoch_025.ckpt', 'datamodule.num_workers=0']
Traceback (most recent call last):
  File "../test.py", line 22, in main
    return test(config)
  File "/home/compu/jh/project/colon_compare/src/testing_pipeline.py", line 67, in test
    trainer.test(model=model, datamodule=datamodule, ckpt_path=config.ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 938, in test
    return self._call_and_handle_interrupt(self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 985, in _test_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 1179, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 1141, in _restore_modules_and_callbacks
    self._checkpoint_connector.restore_model()
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 179, in restore_model
    self.trainer.strategy.load_model_state_dict(self._loaded_checkpoint)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/strategy.py", line 319, in load_model_state_dict
    self.lightning_module.load_state_dict(checkpoint["state_dict"])
  File "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py", line 1497, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ColonLitModule:
	Missing key(s) in state_dict: "model.patch_embed.backbone.stem.conv.weight", "model.patch_embed.backbone.stem.norm.weight", "model.patch_embed.backbone.stem.norm.bias", "model.patch_embed.backbone.stages.0.blocks.0.downsample.conv.weight", "model.patch_embed.backbone.stages.0.blocks.0.downsample.norm.weight", "model.patch_embed.backbone.stages.0.blocks.0.downsample.norm.bias", "model.patch_embed.backbone.stages.0.blocks.0.conv1.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm1.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm1.bias", "model.patch_embed.backbone.stages.0.blocks.0.conv2.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm2.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm2.bias", "model.patch_embed.backbone.stages.0.blocks.0.conv3.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm3.weight", "model.patch_embed.backbone.stages.0.blocks.0.norm3.bias", "model.patch_embed.backbone.stages.0.blocks.1.conv1.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm1.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm1.bias", "model.patch_embed.backbone.stages.0.blocks.1.conv2.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm2.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm2.bias", "model.patch_embed.backbone.stages.0.blocks.1.conv3.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm3.weight", "model.patch_embed.backbone.stages.0.blocks.1.norm3.bias", "model.patch_embed.backbone.stages.0.blocks.2.conv1.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm1.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm1.bias", "model.patch_embed.backbone.stages.0.blocks.2.conv2.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm2.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm2.bias", "model.patch_embed.backbone.stages.0.blocks.2.conv3.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm3.weight", "model.patch_embed.backbone.stages.0.blocks.2.norm3.bias", "model.patch_embed.backbone.stages.1.blocks.0.downsample.conv.weight", "model.patch_embed.backbone.stages.1.blocks.0.downsample.norm.weight", "model.patch_embed.backbone.stages.1.blocks.0.downsample.norm.bias", "model.patch_embed.backbone.stages.1.blocks.0.conv1.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm1.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm1.bias", "model.patch_embed.backbone.stages.1.blocks.0.conv2.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm2.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm2.bias", "model.patch_embed.backbone.stages.1.blocks.0.conv3.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm3.weight", "model.patch_embed.backbone.stages.1.blocks.0.norm3.bias", "model.patch_embed.backbone.stages.1.blocks.1.conv1.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm1.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm1.bias", "model.patch_embed.backbone.stages.1.blocks.1.conv2.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm2.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm2.bias", "model.patch_embed.backbone.stages.1.blocks.1.conv3.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm3.weight", "model.patch_embed.backbone.stages.1.blocks.1.norm3.bias", "model.patch_embed.backbone.stages.1.blocks.2.conv1.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm1.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm1.bias", "model.patch_embed.backbone.stages.1.blocks.2.conv2.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm2.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm2.bias", "model.patch_embed.backbone.stages.1.blocks.2.conv3.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm3.weight", "model.patch_embed.backbone.stages.1.blocks.2.norm3.bias", "model.patch_embed.backbone.stages.1.blocks.3.conv1.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm1.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm1.bias", "model.patch_embed.backbone.stages.1.blocks.3.conv2.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm2.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm2.bias", "model.patch_embed.backbone.stages.1.blocks.3.conv3.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm3.weight", "model.patch_embed.backbone.stages.1.blocks.3.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.0.downsample.conv.weight", "model.patch_embed.backbone.stages.2.blocks.0.downsample.norm.weight", "model.patch_embed.backbone.stages.2.blocks.0.downsample.norm.bias", "model.patch_embed.backbone.stages.2.blocks.0.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.0.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.0.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.0.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.1.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.1.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.1.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.1.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.2.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.2.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.2.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.2.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.3.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.3.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.3.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.3.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.4.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.4.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.4.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.4.norm3.bias", "model.patch_embed.backbone.stages.2.blocks.5.conv1.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm1.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm1.bias", "model.patch_embed.backbone.stages.2.blocks.5.conv2.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm2.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm2.bias", "model.patch_embed.backbone.stages.2.blocks.5.conv3.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm3.weight", "model.patch_embed.backbone.stages.2.blocks.5.norm3.bias", "model.patch_embed.backbone.stages.3.blocks.0.downsample.conv.weight", "model.patch_embed.backbone.stages.3.blocks.0.downsample.norm.weight", "model.patch_embed.backbone.stages.3.blocks.0.downsample.norm.bias", "model.patch_embed.backbone.stages.3.blocks.0.conv1.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm1.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm1.bias", "model.patch_embed.backbone.stages.3.blocks.0.conv2.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm2.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm2.bias", "model.patch_embed.backbone.stages.3.blocks.0.conv3.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm3.weight", "model.patch_embed.backbone.stages.3.blocks.0.norm3.bias", "model.patch_embed.backbone.stages.3.blocks.1.conv1.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm1.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm1.bias", "model.patch_embed.backbone.stages.3.blocks.1.conv2.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm2.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm2.bias", "model.patch_embed.backbone.stages.3.blocks.1.conv3.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm3.weight", "model.patch_embed.backbone.stages.3.blocks.1.norm3.bias", "model.patch_embed.backbone.stages.3.blocks.2.conv1.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm1.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm1.bias", "model.patch_embed.backbone.stages.3.blocks.2.conv2.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm2.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm2.bias", "model.patch_embed.backbone.stages.3.blocks.2.conv3.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm3.weight", "model.patch_embed.backbone.stages.3.blocks.2.norm3.bias", "model.blocks.12.norm1.weight", "model.blocks.12.norm1.bias", "model.blocks.12.attn.qkv.weight", "model.blocks.12.attn.qkv.bias", "model.blocks.12.attn.proj.weight", "model.blocks.12.attn.proj.bias", "model.blocks.12.norm2.weight", "model.blocks.12.norm2.bias", "model.blocks.12.mlp.fc1.weight", "model.blocks.12.mlp.fc1.bias", "model.blocks.12.mlp.fc2.weight", "model.blocks.12.mlp.fc2.bias", "model.blocks.13.norm1.weight", "model.blocks.13.norm1.bias", "model.blocks.13.attn.qkv.weight", "model.blocks.13.attn.qkv.bias", "model.blocks.13.attn.proj.weight", "model.blocks.13.attn.proj.bias", "model.blocks.13.norm2.weight", "model.blocks.13.norm2.bias", "model.blocks.13.mlp.fc1.weight", "model.blocks.13.mlp.fc1.bias", "model.blocks.13.mlp.fc2.weight", "model.blocks.13.mlp.fc2.bias", "model.blocks.14.norm1.weight", "model.blocks.14.norm1.bias", "model.blocks.14.attn.qkv.weight", "model.blocks.14.attn.qkv.bias", "model.blocks.14.attn.proj.weight", "model.blocks.14.attn.proj.bias", "model.blocks.14.norm2.weight", "model.blocks.14.norm2.bias", "model.blocks.14.mlp.fc1.weight", "model.blocks.14.mlp.fc1.bias", "model.blocks.14.mlp.fc2.weight", "model.blocks.14.mlp.fc2.bias", "model.blocks.15.norm1.weight", "model.blocks.15.norm1.bias", "model.blocks.15.attn.qkv.weight", "model.blocks.15.attn.qkv.bias", "model.blocks.15.attn.proj.weight", "model.blocks.15.attn.proj.bias", "model.blocks.15.norm2.weight", "model.blocks.15.norm2.bias", "model.blocks.15.mlp.fc1.weight", "model.blocks.15.mlp.fc1.bias", "model.blocks.15.mlp.fc2.weight", "model.blocks.15.mlp.fc2.bias", "model.blocks.16.norm1.weight", "model.blocks.16.norm1.bias", "model.blocks.16.attn.qkv.weight", "model.blocks.16.attn.qkv.bias", "model.blocks.16.attn.proj.weight", "model.blocks.16.attn.proj.bias", "model.blocks.16.norm2.weight", "model.blocks.16.norm2.bias", "model.blocks.16.mlp.fc1.weight", "model.blocks.16.mlp.fc1.bias", "model.blocks.16.mlp.fc2.weight", "model.blocks.16.mlp.fc2.bias", "model.blocks.17.norm1.weight", "model.blocks.17.norm1.bias", "model.blocks.17.attn.qkv.weight", "model.blocks.17.attn.qkv.bias", "model.blocks.17.attn.proj.weight", "model.blocks.17.attn.proj.bias", "model.blocks.17.norm2.weight", "model.blocks.17.norm2.bias", "model.blocks.17.mlp.fc1.weight", "model.blocks.17.mlp.fc1.bias", "model.blocks.17.mlp.fc2.weight", "model.blocks.17.mlp.fc2.bias", "model.blocks.18.norm1.weight", "model.blocks.18.norm1.bias", "model.blocks.18.attn.qkv.weight", "model.blocks.18.attn.qkv.bias", "model.blocks.18.attn.proj.weight", "model.blocks.18.attn.proj.bias", "model.blocks.18.norm2.weight", "model.blocks.18.norm2.bias", "model.blocks.18.mlp.fc1.weight", "model.blocks.18.mlp.fc1.bias", "model.blocks.18.mlp.fc2.weight", "model.blocks.18.mlp.fc2.bias", "model.blocks.19.norm1.weight", "model.blocks.19.norm1.bias", "model.blocks.19.attn.qkv.weight", "model.blocks.19.attn.qkv.bias", "model.blocks.19.attn.proj.weight", "model.blocks.19.attn.proj.bias", "model.blocks.19.norm2.weight", "model.blocks.19.norm2.bias", "model.blocks.19.mlp.fc1.weight", "model.blocks.19.mlp.fc1.bias", "model.blocks.19.mlp.fc2.weight", "model.blocks.19.mlp.fc2.bias", "model.blocks.20.norm1.weight", "model.blocks.20.norm1.bias", "model.blocks.20.attn.qkv.weight", "model.blocks.20.attn.qkv.bias", "model.blocks.20.attn.proj.weight", "model.blocks.20.attn.proj.bias", "model.blocks.20.norm2.weight", "model.blocks.20.norm2.bias", "model.blocks.20.mlp.fc1.weight", "model.blocks.20.mlp.fc1.bias", "model.blocks.20.mlp.fc2.weight", "model.blocks.20.mlp.fc2.bias", "model.blocks.21.norm1.weight", "model.blocks.21.norm1.bias", "model.blocks.21.attn.qkv.weight", "model.blocks.21.attn.qkv.bias", "model.blocks.21.attn.proj.weight", "model.blocks.21.attn.proj.bias", "model.blocks.21.norm2.weight", "model.blocks.21.norm2.bias", "model.blocks.21.mlp.fc1.weight", "model.blocks.21.mlp.fc1.bias", "model.blocks.21.mlp.fc2.weight", "model.blocks.21.mlp.fc2.bias", "model.blocks.22.norm1.weight", "model.blocks.22.norm1.bias", "model.blocks.22.attn.qkv.weight", "model.blocks.22.attn.qkv.bias", "model.blocks.22.attn.proj.weight", "model.blocks.22.attn.proj.bias", "model.blocks.22.norm2.weight", "model.blocks.22.norm2.bias", "model.blocks.22.mlp.fc1.weight", "model.blocks.22.mlp.fc1.bias", "model.blocks.22.mlp.fc2.weight", "model.blocks.22.mlp.fc2.bias", "model.blocks.23.norm1.weight", "model.blocks.23.norm1.bias", "model.blocks.23.attn.qkv.weight", "model.blocks.23.attn.qkv.bias", "model.blocks.23.attn.proj.weight", "model.blocks.23.attn.proj.bias", "model.blocks.23.norm2.weight", "model.blocks.23.norm2.bias", "model.blocks.23.mlp.fc1.weight", "model.blocks.23.mlp.fc1.bias", "model.blocks.23.mlp.fc2.weight", "model.blocks.23.mlp.fc2.bias".
	size mismatch for model.cls_token: copying a param with shape torch.Size([1, 1, 192]) from checkpoint, the shape in current model is torch.Size([1, 1, 1024]).
	size mismatch for model.pos_embed: copying a param with shape torch.Size([1, 577, 192]) from checkpoint, the shape in current model is torch.Size([1, 145, 1024]).
	size mismatch for model.patch_embed.proj.weight: copying a param with shape torch.Size([192, 3, 16, 16]) from checkpoint, the shape in current model is torch.Size([1024, 2048, 1, 1]).
	size mismatch for model.patch_embed.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.0.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.0.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.0.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.0.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.0.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.0.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.0.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.1.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.1.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.1.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.1.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.1.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.1.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.1.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.2.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.2.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.2.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.2.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.2.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.2.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.2.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.3.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.3.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.3.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.3.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.3.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.3.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.3.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.4.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.4.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.4.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.4.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.4.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.4.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.4.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.5.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.5.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.5.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.5.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.5.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.5.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.5.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.6.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.6.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.6.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.6.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.6.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.6.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.6.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.7.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.7.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.7.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.7.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.7.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.7.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.7.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.8.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.8.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.8.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.8.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.8.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.8.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.8.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.9.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.9.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.9.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.9.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.9.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.9.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.9.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.10.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.10.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.10.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.10.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.10.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.10.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.10.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.norm1.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.norm1.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.attn.qkv.weight: copying a param with shape torch.Size([576, 192]) from checkpoint, the shape in current model is torch.Size([3072, 1024]).
	size mismatch for model.blocks.11.attn.qkv.bias: copying a param with shape torch.Size([576]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for model.blocks.11.attn.proj.weight: copying a param with shape torch.Size([192, 192]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).
	size mismatch for model.blocks.11.attn.proj.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.norm2.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.norm2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.blocks.11.mlp.fc1.weight: copying a param with shape torch.Size([768, 192]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).
	size mismatch for model.blocks.11.mlp.fc1.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([4096]).
	size mismatch for model.blocks.11.mlp.fc2.weight: copying a param with shape torch.Size([192, 768]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).
	size mismatch for model.blocks.11.mlp.fc2.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.norm.weight: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.norm.bias: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([1024]).
	size mismatch for model.head.weight: copying a param with shape torch.Size([4, 192]) from checkpoint, the shape in current model is torch.Size([4, 1024]).
	size mismatch for discriminator_layer1.0.weight: copying a param with shape torch.Size([512, 192]) from checkpoint, the shape in current model is torch.Size([512, 1024]).
	size mismatch for discriminator_layer2.0.weight: copying a param with shape torch.Size([512, 384]) from checkpoint, the shape in current model is torch.Size([512, 2048]).
