[[36m2022-08-24 12:52:36,896[39m][[34msrc.testing_pipeline[39m][[32mINFO[39m] - Instantiating trainer <pytorch_lightning.Trainer>
[[36m2022-08-24 12:52:36,901[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Using 16bit native Automatic Mixed Precision (AMP)
[[36m2022-08-24 12:52:36,903[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - GPU available: True, used: True
[[36m2022-08-24 12:52:36,903[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - TPU available: False, using: 0 TPU cores
[[36m2022-08-24 12:52:36,903[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - IPU available: False, using: 0 IPUs
[[36m2022-08-24 12:52:36,904[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - HPU available: False, using: 0 HPUs
[[36m2022-08-24 12:52:36,904[39m][[34msrc.testing_pipeline[39m][[32mINFO[39m] - Starting testing!
[[36m2022-08-24 12:52:44,458[39m][[34mpytorch_lightning.utilities.seed[39m][[32mINFO[39m] - Global seed set to 42
[[36m2022-08-24 12:52:44,462[39m][[34mpytorch_lightning.utilities.distributed[39m][[32mINFO[39m] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/3
[[36m2022-08-24 12:53:38,661[39m][[34mtorch.distributed.distributed_c10d[39m][[32mINFO[39m] - Added key: store_based_barrier_key:1 to store for rank: 0
[[36m2022-08-24 12:53:38,662[39m][[34mtorch.distributed.distributed_c10d[39m][[32mINFO[39m] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 3 nodes.
[[36m2022-08-24 12:53:38,662[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 3 processes
----------------------------------------------------------------------------------------------------
[[36m2022-08-24 12:54:08,670[39m][[34mpytorch_lightning.utilities.rank_zero[39m][[32mINFO[39m] - Restoring states from the checkpoint path at /home/compu/jh/project/colon_compare/scripts/
Error executing job with overrides: ['datamodule.data_ratio=0.1', 'model.name=vit_large_r50_s32_384', 'model=voting.yaml', 'logger.wandb.project=colon_new', 'trainer.devices=[1,2,3]', 'datamodule=colon.yaml', 'logger.wandb.tags=[voting,ent,10%]', 'ckpt_path=', 'model.key=ent', 'model.threshold=0.25', 'model.sampling=random']
Traceback (most recent call last):
  File "../test.py", line 23, in main
    return test(config)
  File "/home/compu/jh/project/colon_compare/src/testing_pipeline.py", line 78, in test
    trainer.test(model=model, datamodule=datamodule, ckpt_path=config.ckpt_path,)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 938, in test
    return self._call_and_handle_interrupt(self._test_impl, model, dataloaders, ckpt_path, verbose, datamodule)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 721, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 985, in _test_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 1179, in _run
    self._restore_modules_and_callbacks(ckpt_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py", line 1139, in _restore_modules_and_callbacks
    self._checkpoint_connector.resume_start(checkpoint_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 84, in resume_start
    self._loaded_checkpoint = self._load_and_validate_checkpoint(checkpoint_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py", line 88, in _load_and_validate_checkpoint
    loaded_checkpoint = self.trainer.strategy.load_checkpoint(checkpoint_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/strategies/strategy.py", line 316, in load_checkpoint
    return self.checkpoint_io.load_checkpoint(checkpoint_path)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/io/torch_plugin.py", line 85, in load_checkpoint
    return pl_load(path, map_location=map_location)
  File "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/cloud_io.py", line 46, in load
    with fs.open(path_or_url, "rb") as f:
  File "/usr/local/lib/python3.8/dist-packages/fsspec/spec.py", line 1009, in open
    f = self._open(
  File "/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py", line 155, in _open
    return LocalFileOpener(path, mode, fs=self, **kwargs)
  File "/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py", line 250, in __init__
    self._open()
  File "/usr/local/lib/python3.8/dist-packages/fsspec/implementations/local.py", line 255, in _open
    self.f = open(self.path, mode=self.mode)
IsADirectoryError: [Errno 21] Is a directory: '/home/compu/jh/project/colon_compare/scripts'
Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.